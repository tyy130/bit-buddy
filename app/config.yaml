# app/config.yaml
server:
  host: "127.0.0.1"
  port: 11434

llm:
  # provider: "llamacpp" or "ollama"
  provider: "llamacpp"
  # Only used for llama.cpp server:
  # Change to match your model filename under /models
  llamacpp:
    base_url: "http://127.0.0.1:8080"   # llama.cpp server --api --port 8080
    model: "models/phi-3-mini-4k-instruct.Q4_K_M.gguf"
  # Only used for Ollama:
  ollama:
    base_url: "http://127.0.0.1:11434"
    model: "qwen2.5:1.5b-instruct"

embedder:
  model: "bge-small-en"     # FastEmbed offline model
  dim: 384                  # embedding dimension for bge-small-en

index:
  dir: "index"
  kb_dir: "kb"
  chunk_chars: 1200
  chunk_overlap: 200
